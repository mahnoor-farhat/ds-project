{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bbdb61",
   "metadata": {},
   "source": [
    "Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3021357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Bidirectional, Concatenate\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading stopwords & punkt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f8c65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the preprocessed dataset\n",
    "df = pd.read_parquet(\"preprocessed_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb92acf",
   "metadata": {},
   "source": [
    "Tokenization & Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f02180c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000  # Maximum number of words in tokenizer\n",
    "maxlen_articles = 400  # Maximum length of articles\n",
    "maxlen_abstracts = 100  # Maximum length of abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9f3677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_articles = Tokenizer(num_words=max_features)\n",
    "tokenizer_articles.fit_on_texts(df['article'])\n",
    "sequences_articles = tokenizer_articles.texts_to_sequences(df['article'])\n",
    "tokenized_articles = pad_sequences(sequences_articles, maxlen=maxlen_articles, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc59b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_abstracts = Tokenizer(num_words=max_features)\n",
    "tokenizer_abstracts.fit_on_texts(df['abstract'])\n",
    "sequences_abstracts = tokenizer_abstracts.texts_to_sequences(df['abstract'])\n",
    "tokenized_abstracts = pad_sequences(sequences_abstracts, maxlen=maxlen_abstracts, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2a348",
   "metadata": {},
   "source": [
    "LSTM - Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfcf90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(tokenized_articles, tokenized_abstracts, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea3f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model architecture\n",
    "embedding_dim = 100  # Dimension of word embeddings\n",
    "latent_dim = 300  # Dimension of LSTM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "113e3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder(in which the input sequence and produces a context vector [or hidden state], \n",
    "# which summarizes the input information and is passed to the decoder)\n",
    "\n",
    "## Defining the encoding layers\n",
    "encoder_inputs = Input(shape=(maxlen_articles,))\n",
    "enc_emb = Embedding(max_features, embedding_dim, trainable=True)(encoder_inputs)\n",
    "encoder_lstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True))\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(enc_emb)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f3bedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder(in which the output sequence, one token at a time, \n",
    "# using the context vector from the encoder and its own previous outputs)\n",
    "\n",
    "## Defining the decoding layers\n",
    "decoder_inputs = Input(shape=(maxlen_abstracts,))\n",
    "dec_emb_layer = Embedding(max_features, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "decoder_dense = Dense(max_features, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1c124cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aeaf8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with sparse categorical crossentropy loss\n",
    "model.compile(optimizer='adam', loss=sparse_categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7a8346c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 400)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 400, 100)     500000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  [(None, 400, 600),   962400      ['embedding[0][0]']              \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 100, 100)     500000      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 600)          0           ['bidirectional[0][1]',          \n",
      "                                                                  'bidirectional[0][3]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 600)          0           ['bidirectional[0][2]',          \n",
      "                                                                  'bidirectional[0][4]']          \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 100, 600),   1682400     ['embedding_1[0][0]',            \n",
      "                                 (None, 600),                     'concatenate[0][0]',            \n",
      "                                 (None, 600)]                     'concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 100, 5000)    3005000     ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,649,800\n",
      "Trainable params: 6,649,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8f81c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 25\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eb17e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping is used to halt training when the validation loss stops improving, helping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebe8bbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes after padding:\n",
      "X_train_padded shape: (8000, 400)\n",
      "y_train_padded_input shape: (8000, 100)\n",
      "y_train_padded_target shape: (8000, 100)\n",
      "X_val_padded shape: (2000, 400)\n",
      "y_val_padded_input shape: (2000, 100)\n",
      "y_val_padded_target shape: (2000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Adjust maxlen_abstracts and maxlen_articles if necessary\n",
    "maxlen_abstracts = 100\n",
    "maxlen_articles = 400\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequences(X_train, maxlen=maxlen_articles, padding='post')\n",
    "X_val_padded = pad_sequences(X_val, maxlen=maxlen_articles, padding='post')\n",
    "\n",
    "y_train_padded_input = pad_sequences(y_train[:, :-1], maxlen=maxlen_abstracts, padding='post')\n",
    "y_train_padded_target = pad_sequences(y_train[:, 1:], maxlen=maxlen_abstracts, padding='post')\n",
    "\n",
    "y_val_padded_input = pad_sequences(y_val[:, :-1], maxlen=maxlen_abstracts, padding='post')\n",
    "y_val_padded_target = pad_sequences(y_val[:, 1:], maxlen=maxlen_abstracts, padding='post')\n",
    "\n",
    "# Example shapes check\n",
    "print(\"Shapes after padding:\")\n",
    "print(f\"X_train_padded shape: {X_train_padded.shape}\")\n",
    "print(f\"y_train_padded_input shape: {y_train_padded_input.shape}\")\n",
    "print(f\"y_train_padded_target shape: {y_train_padded_target.shape}\")\n",
    "print(f\"X_val_padded shape: {X_val_padded.shape}\")\n",
    "print(f\"y_val_padded_input shape: {y_val_padded_input.shape}\")\n",
    "print(f\"y_val_padded_target shape: {y_val_padded_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "642d4154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 2510s 10s/step - loss: 6.2108 - accuracy: 0.1333 - val_loss: 5.8865 - val_accuracy: 0.1639\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 3617s 14s/step - loss: 5.6843 - accuracy: 0.1827 - val_loss: 5.5317 - val_accuracy: 0.1994\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 2761s 11s/step - loss: 5.3556 - accuracy: 0.2142 - val_loss: 5.2731 - val_accuracy: 0.2205\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 2954s 12s/step - loss: 5.1236 - accuracy: 0.2305 - val_loss: 5.1032 - val_accuracy: 0.2329\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 3315s 13s/step - loss: 4.9468 - accuracy: 0.2414 - val_loss: 4.9818 - val_accuracy: 0.2402\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 2630s 11s/step - loss: 4.7976 - accuracy: 0.2492 - val_loss: 4.8753 - val_accuracy: 0.2475\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 3305s 13s/step - loss: 4.6679 - accuracy: 0.2564 - val_loss: 4.8027 - val_accuracy: 0.2524\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 6492s 26s/step - loss: 4.5577 - accuracy: 0.2629 - val_loss: 4.7477 - val_accuracy: 0.2562\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 21268s 85s/step - loss: 4.4595 - accuracy: 0.2685 - val_loss: 4.7029 - val_accuracy: 0.2596\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 5852s 23s/step - loss: 4.3713 - accuracy: 0.2735 - val_loss: 4.6725 - val_accuracy: 0.2620\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 49350s 198s/step - loss: 4.2851 - accuracy: 0.2787 - val_loss: 4.6483 - val_accuracy: 0.2638\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 50242s 202s/step - loss: 4.2036 - accuracy: 0.2831 - val_loss: 4.6321 - val_accuracy: 0.2662\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 28715s 115s/step - loss: 4.1271 - accuracy: 0.2880 - val_loss: 4.6257 - val_accuracy: 0.2672\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 37826s 152s/step - loss: 4.0538 - accuracy: 0.2920 - val_loss: 4.6185 - val_accuracy: 0.2687\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 2857s 11s/step - loss: 3.9824 - accuracy: 0.2969 - val_loss: 4.6182 - val_accuracy: 0.2697\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 2652s 11s/step - loss: 3.9129 - accuracy: 0.3016 - val_loss: 4.6219 - val_accuracy: 0.2698\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 2670s 11s/step - loss: 3.8456 - accuracy: 0.3061 - val_loss: 4.6270 - val_accuracy: 0.2709\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - ETA: 0s - loss: 3.7806 - accuracy: 0.3108 Restoring model weights from the end of the best epoch: 15.\n",
      "250/250 [==============================] - 2695s 11s/step - loss: 3.7806 - accuracy: 0.3108 - val_loss: 4.6363 - val_accuracy: 0.2710\n",
      "Epoch 18: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_padded, y_train_padded_input],         # Input: X_train_padded and y_train_padded_input\n",
    "    y_train_padded_target,                          # Target: y_train_padded_target\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping],\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(\n",
    "        [X_val_padded, y_val_padded_input],         # Validation input: X_val_padded and y_val_padded_input\n",
    "        y_val_padded_target                         # Validation target: y_val_padded_target\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a93eece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss: 4.618181228637695, Accuracy: 0.2696850001811981\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on validation data\n",
    "loss, accuracy = model.evaluate([X_val_padded, y_val_padded_input], y_val_padded_target, verbose=0)\n",
    "\n",
    "print(f'Evaluation loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f2b6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('text_summarization_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cdb9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the article & abstract tokenizers as pickle file\n",
    "with open('tokenizer_articles.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer_articles, f)\n",
    "\n",
    "with open('tokenizer_abstracts.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer_abstracts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef43ccb",
   "metadata": {},
   "source": [
    "Testing Sample Data (Test Case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da1b547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 154\n",
      "Index 1: Word 'of'\n",
      "Index 2: Word 'the'\n",
      "Index 3: Word 'small'\n",
      "Index 4: Word 'bowel'\n",
      "Index 5: Word 'in'\n",
      "Index 6: Word 'occur'\n",
      "Index 7: Word 'volvulus'\n",
      "Index 8: Word 'lymphangioma'\n",
      "Index 9: Word 'cystic'\n",
      "Index 10: Word 'strain'\n"
     ]
    }
   ],
   "source": [
    "# Example text data\n",
    "texts = [\n",
    "    \"abdominal cystic lymphangiomas rare occur secondary congenital malformation lymphatics mostly mesenterium acute chronic volvulus small bowel may occur traction lymphangioma transverse supraumbilical laparotomy performed volvulus small bowel seen lead point volvulus seven cm benign cystic lymphangioma located fifteen cm distal treitz ligament vital bowel repositioned cyst resected including small section jejunum anastomosed end end\",\n",
    "    \"key clinical messageabdominal cystic lymphangiomas are rare and occur secondary to congenital malformation of the lymphatics mostly in the mesenterium acute or chronic volvulus of the small bowel may occur by traction of the lymphangioma therapy includes resection of the lymphangioma and of the small bowel involved\",\n",
    "    \"maldi tof ms spectrum bacillus massiliogabonensis available hundred fifty six titre urms database rrna gene sequence strain marseille deposited genbank database accession number lt strain marseille deposited collection de souches de lunit des rickettsies registered number\",\n",
    "    \"the discovery of new bacteria from the human gut using culturomics method is novel field of increasing interest in microbiology here the main characteristics of bacillus massiliogabonensis strain marseille new gram negative bacterium isolated from the stool sample of healthy sixteen year old gabonese boy are reported\",\n",
    "    \"authors report conflicts interest authors alone responsible content writing article\",\n",
    "    \"abstractthe exposure of prosthetic vascular graft is dangerous complication in revascularization procedures in this case report we describe successful coverage of an exposed prosthetic femorofemoral vascular graft in the suprapubic area with vertical rectus abdominis myocutaneous island flap\"\n",
    "]\n",
    "\n",
    "# Initialize and fit the tokenizer on your text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Generate index_to_word dictionary\n",
    "index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "\n",
    "# Save the tokenizer and index_to_word dictionary\n",
    "with open('tokenizer_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open('index_to_word.pkl', 'wb') as f:\n",
    "    pickle.dump(index_to_word, f)\n",
    "\n",
    "# Verify the contents\n",
    "print(f\"Vocabulary Size: {len(tokenizer.word_index)}\")\n",
    "for i, (index, word) in enumerate(index_to_word.items()):\n",
    "    if i < 10:  # Print first 10 entries\n",
    "        print(f\"Index {index}: Word '{word}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f50b1fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 775ms/step\n",
      "Original Article: abdominal cystic lymphangiomas rare occur secondary congenital malformation lymphatics mostly mesenterium acute chronic volvulus small bowel may occur traction lymphangioma transverse supraumbilical laparotomy performed volvulus small bowel seen lead point volvulus seven cm benign cystic lymphangioma located fifteen cm distal treitz ligament vital bowel repositioned cyst resected including small section jejunum anastomosed end end\n",
      "Generated Summary: [[[3.5144818e-05 1.1678024e-03 1.7694887e-02 ... 2.3339448e-05\n",
      "   1.3893372e-05 1.7346711e-06]\n",
      "  [1.7188948e-04 5.1575415e-03 1.1363861e-02 ... 7.3858696e-06\n",
      "   7.9560930e-05 9.5291955e-07]\n",
      "  [2.8256234e-05 3.7976238e-03 1.7489752e-02 ... 2.9629905e-06\n",
      "   3.1072301e-05 6.8925465e-07]\n",
      "  ...\n",
      "  [9.9995112e-01 6.2463062e-07 4.1071835e-06 ... 6.8799607e-14\n",
      "   4.4407125e-15 1.5501881e-11]\n",
      "  [9.9995124e-01 6.1989510e-07 4.0945865e-06 ... 6.8927808e-14\n",
      "   4.4086604e-15 1.5542385e-11]\n",
      "  [9.9995136e-01 6.1284385e-07 4.0793011e-06 ... 6.9156168e-14\n",
      "   4.3544405e-15 1.5612773e-11]]]\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(predicted_summary):\n",
    "    # Assuming predicted_summary is in text format\n",
    "    return predicted_summary  # Adjust based on actual output format\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model('text_summarization_model.h5', custom_objects={'_TextVectorization': TextVectorization})\n",
    "\n",
    "# Tokenize text\n",
    "tokenized_article = tokenizer_articles.texts_to_sequences([article])\n",
    "tokenized_article = pad_sequences(tokenized_article, maxlen=max_len_articles, padding='post')\n",
    "\n",
    "tokenized_abstract = tokenizer_abstracts.texts_to_sequences([abstract])\n",
    "tokenized_abstract = pad_sequences(tokenized_abstract, maxlen=max_len_abstracts, padding='post')\n",
    "\n",
    "# Predict the summary\n",
    "predicted_summary = model.predict([tokenized_article, tokenized_abstract])\n",
    "\n",
    "\n",
    "print(\"Original Article:\", article)\n",
    "print(\"Generated Summary:\", predicted_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "007d59c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 800ms/step\n",
      "Original Article: abdominal cystic lymphangiomas are rare and occur secondary to congenital malformation lymphatics mostly in the mesenterium acute or chronic volvulus of the small bowel may occur by traction of the lymphangioma therapy includes resection of the lymphangioma and of the small bowel involved\n",
      "Generated Summary: point strain strain  island  of bowel in of  strain of   of   bowel of of   small are bowel of the of strain strain of the of seven of of strain  small bowel\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers\n",
    "with open('tokenizer_articles.pkl', 'rb') as f:\n",
    "    tokenizer_articles = pickle.load(f)\n",
    "\n",
    "with open('tokenizer_abstracts.pkl', 'rb') as f:\n",
    "    tokenizer_abstracts = pickle.load(f)\n",
    "\n",
    "# Define or load your index_to_word dictionary\n",
    "with open('index_to_word.pkl', 'rb') as f:\n",
    "    index_to_word = pickle.load(f)\n",
    "\n",
    "# Define padding token if applicable\n",
    "padding_token = 0  # Adjust based on your model's padding token index\n",
    "\n",
    "# Function to decode the predicted summary\n",
    "def decode_sequence(predicted_summary, index_to_word, padding_token=0):\n",
    "    # Get the index of the maximum probability for each token position\n",
    "    token_indices = np.argmax(predicted_summary, axis=-1)\n",
    "    \n",
    "    # Convert token indices to words, excluding padding tokens\n",
    "    decoded_summary = ' '.join([index_to_word.get(idx, '') for idx in token_indices[0] if idx != padding_token])\n",
    "    \n",
    "    return decoded_summary\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('text_summarization_model.h5', custom_objects={'_TextVectorization': TextVectorization})\n",
    "\n",
    "# Example text\n",
    "article = \"abdominal cystic lymphangiomas are rare and occur secondary to congenital malformation lymphatics mostly in the mesenterium acute or chronic volvulus of the small bowel may occur by traction of the lymphangioma therapy includes resection of the lymphangioma and of the small bowel involved\"\n",
    "abstract = \"key clinical message abdominal cystic lymphangiomas are rare and occur secondary to congenital malformation of the lymphatics mostly in the mesenterium acute or chronic volvulus of the small bowel may occur by traction of the lymphangioma therapy includes resection of the lymphangioma and of the small bowel involved\"\n",
    "\n",
    "# Tokenize text\n",
    "max_len_articles = 400  # Set the max length based on your model\n",
    "max_len_abstracts = 100  # Set the max length based on your model\n",
    "\n",
    "tokenized_article = tokenizer_articles.texts_to_sequences([article])\n",
    "tokenized_article = pad_sequences(tokenized_article, maxlen=max_len_articles, padding='post')\n",
    "\n",
    "tokenized_abstract = tokenizer_abstracts.texts_to_sequences([abstract])\n",
    "tokenized_abstract = pad_sequences(tokenized_abstract, maxlen=max_len_abstracts, padding='post')\n",
    "\n",
    "# Predict the summary\n",
    "predicted_summary = model.predict([tokenized_article, tokenized_abstract])\n",
    "\n",
    "# Decode the predicted summary\n",
    "decoded_summary = decode_sequence(predicted_summary, index_to_word, padding_token)\n",
    "\n",
    "print(\"Original Article:\", article)\n",
    "print(\"Generated Summary:\", decoded_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
